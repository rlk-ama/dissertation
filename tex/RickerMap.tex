\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[fleqn]{amsmath}
\usepackage{setspace}
\usepackage[T1]{fontenc}
\usepackage{parskip}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{natbib}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{array}
\usepackage{bbold}
\usepackage{amssymb}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{graphicx}
\graphicspath{ {figures/} }
\usepackage{titlesec}
\usepackage{titling}
\usepackage{cleveref}
\usepackage[toc,page]{appendix}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
	\savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
	\savebox{\mysim}{\hbox{$\sim$}}%
	\mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\onehalfspacing

\setlength{\droptitle}{-10em}

\title{Monte Carlo Methods for Bayesian Inference on Two Population
	Dynamics Models: The Ricker Map and Nicholson's Sheep Blowfly Experiments}
\date{}

\begin{document}
	\maketitle 
	\thispagestyle{empty}
	\begin{center}
		\vspace{-5mm}
		\includegraphics[scale=1]{logo.png} \\
		\vspace{20mm}
		{\large Raphaël Lopez-Kaufman} \\
		\vspace{2mm}
		{\large Oriel College, University of Oxford} \\
		\vspace{40mm}
		A dissertation submitted in partial fulfilment of the requirements for the degree of \\
		\vspace{1mm}
		\textit{Master of Science in Applied Statistics} \\
		\vspace{1mm}
		Trinity 2015
	\end{center}
	
	\vspace{25 mm}
	\definecolor{keywords}{RGB}{255,0,90}
	\definecolor{comments}{RGB}{0,0,113}
	\definecolor{red}{RGB}{160,0,0}
	\definecolor{green}{RGB}{0,150,0}
	
	\begin{abstract}
		We investigate in this dissertation the use of a class of Particle Markov Chain Monte Carlo (PMCMC) algorithm, the Particle Marginal Metropolis Hastings sampler (PMMH), to perform bayesian inference on ecological data, and more particularly on population dynamics. We focused more specifically on two famous models. The first one is the noisily observed Ricker map (NRM). It is a state space model built around the Ricker map (one of the first and more general model to describe complex population dynamics) to account for observational and process noise. The second one is a model proposed by Wood~\cite{wood2010statistical} to solve the stochastic version of the differential equation which describes Nicholson's sheep blowfly experiments data. These two models are challenging from the inference perspective since, depending on the values of the parameters, they can describe linear, periodic and chaotic population size evolutions. We expanded on the work of Fasiolo et al.\cite{fasiolo2014statistical} to develop PMMH algorithms based on specifically tailored proposal distributions to propagate particles in the particle filter used to approximate the true intractable likelihood. Our algorithm for the NRM model gave, on synthetic data, results similar to those of Fasiolo in term of accuracy, but for a much cheaper computational cost. We also tested our sampler on real data from the Global Population Dynamic Database (GPDD). 
	\end{abstract}
	
	\newpage
	\vspace*{80mm}
		\textit{I would like to express my gratitude to my academic supervisors Professor Arnaud Doucet and Doctor Lawrence Murray from University of Oxford for their time and guidance}
	
	\newpage
	
	\listoffigures
	\clearpage
	\listoftables
	\clearpage
	\tableofcontents
	
	\clearpage
	\section{Introduction}
	The study of population dynamics, either by ecologists or epidemiologists, often requires the use of elaborate methods. For example, King et al.~\cite{king2008inapparent} and Bhadra et al.~\cite{bhadra2011malaria} studied the spread of epidemics, cholera and malaria, both using iterated filtering for parameter inference of state space models~\cite{ionides2006inference}. The necessity to design such complex inference strategies arises as a consequence of the fact that most of the time population dynamics models are chaotic, or nearly chaotic resulting in multidimensional and multimodal likelihoods. Therefore traditional inference based on numerical methods to find maximum likelihood estimates do not yield satisfactory results. Chaotic behaviour in ecological and epidemiological system is not only of theoretical interest as practical examples abound (see Kausrud et al.~\cite{kausrud2008linking} on lemmings and Anderson et al.~\cite{anderson2008fishing} on fish). \\
	
	Bayesian approaches to inference have become increasingly popular with the recent improvements in computing power. Particle Markov Chain Monte Carlo Methods (PMCMC)~\cite{andrieu2010particle}, which combine a standard Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) algorithms has become a method of choice for inference on such difficult models (see Losa et al.~\cite{losa2003sequential}, Dowd~\cite{dowd2006sequential} and Jones et al.~\cite{jones2010bayesian} for examples in marine ecology). \\
	
	Another recent type of approach, termed likelihood-free or Approximate Bayesian Computation (ABC), and first described by Rubin~\cite{rubin1984bayesianly} has also become popular (see Butler et al.~\cite{butler57latent} for an example in ecology, Tanaka et al.~\cite{tanaka2006using} in epidemiology and Thornton et al.~\cite{thornton2006approximate} in biology). This method consists in simulation from the likelihood when it is impossible to evaluate it, even point wise, as is required in PMCMC. Techniques combining MCMC and ABC (ABC-MCMC) or SMC and ABC (ABC-SMC) to sample from the posterior distribution of the parameters of such models are described respectively in Marjoram et al.~\cite{marjoram2003markov} and Toni et al.~\cite{toni2009approximate}. For a complete review see Marin et al.~\cite{marin2012approximate}. \\
	
	We studied two very famous examples of chaotic population dynamics models from ecology and performed bayesian inference on the parameters of these models using respectively a Particle Marginal Metropolis Hastings (PMMH) sampler as described in Adrieu et al.~\cite{andrieu2010particle}, which is one of the PMCMC algorithms, and a ABC-MCMC sampler. The first model is a noisily observed version of the Ricker map described by Wood~\cite{wood2010statistical} whereas the second one is a solution to the stochastic differential equation suggested by Gurney et al.~\cite{gurney1980nicholson} and Nisbet et al.~\cite{nisbet1982modelling} to describe the last three replicates of the four runs of Nicholson’s classic experiments on sheep blowfly~\cite{nicholson1954outline}~\cite{nicholson1957self}.
	
	In section 1 of this dissertation a overview of the mathematical background needed to understand the difficulties intrinsic to these two population dynamics models will be given. Section 2 describes the statistical methods used to perform bayesian inference on these two models along with the theoretical reasons, advantages and disadvantages of doing so. Section 3 is dedicated to the actual algorithms that were implemented. Section 4 presents the results attained and contrast them with those obtained by Fasiolo~\cite{fasiolo2014statistical}. Finally a conclusion and a discussion on the merits and limitations of the methods presented is given in the last section.
	
	\section{Two ecological models}
	\subsection{The Ricker Map}  \label{rickerGen}
	\subsubsection{The Deterministic Ricker Map}
	The Ricker map is a difference equation used to describe the population dynamics of a wide range of ecological populations. It was first described in a seminal paper by Ricker~\cite{Ricker1954} to account for fish population sizes in fisheries. \\
	If we denote by $N_t$ the size of a population a time $t$, Ricker established that if:
	\begin{itemize}
		\item the average offspring size per individual per unit time is a constant number $r > 0$
		\item there is a crowding effect which reduces by a factor $e^{-\frac{N_t}{K}}$ the offspring size where $K > 0$
		\item generations do not overlap
	\end{itemize}
	then 
	\begin{equation}
		N_{t+1} = r N_t e^{-\frac{N_t}{K}} = f(N_t)
		\label{eq:ricker}
	\end{equation}
	The fact that generations do no overlap, which is generally a strong assumption in biology, is acceptable in the case of seasonally breeding populations, which are widespread in ecology. \\
	This model has very complex dynamics depending on the values of the parameter $r$. It has become a classic discrete population model, and although not taking into account any of the exterior factors which influence greatly ecological populations (such as destruction of natural ecosystems, pervading pollution, etc ...), it provides an accurate description of many experimental population dynamics (see Krkovsek et al. ~\cite{krkovsek2007declining} or Mueter et al.~\cite{mueter2002opposite} for applications of this model to salmon populations and Gao et al.~\cite{gao2012bayesian} to sixteen representative species from the Global Population Dynamics Database (GPDD)). \\
	
	To understand why estimating the parameters of this model given experimental data is not trivial, we first describe its chaotic behaviour. Equation~\ref{eq:ricker} has two equilibria, $N_{eq, 1} = 0$ and $N_{eq, 2} = K\log r$, which are the solutions of  $N_{eq} = r N_{eq} e^{-\frac{N_{eq}}{K}}$. Linearisation around these two equilibria, give $N_{t+1} - N_{eq, 1} = r(N_{t} - N_{eq, 1})$ and $N_{t+1} - N_{eq, 2} = (1-\log r)(N_{t} - N_{eq, 1})$. Therefore $N_{eq, 1}$ is stable when $0 < r < 1$ and unstable when $r > 1$ and $N_{eq, 2}$ is stable when $1 < r < e^2$ and unstable when $r < 1$ or $r > e^2$. The corresponding bifurcation diagram, with $K$ fixed and equal to 1, is shown in Figure~\ref{fig:stability}. Figure~\ref{fig:stab} shows the convergence towards these two equilibria for respectively $r=0.5$ and $r=3$, with $K=10$ in both cases. It can be noticed that the non zero equilibrium value is close to its theoretical value of $10 \log 3 = 10.9$. \\
	Another interesting characteristic of this map, from an ecological point of view, and the reason why it was so widely adopted, resides in the fact that it accounts for scenarii where populations oscillates before reaching an equilibrium. Figure~\ref{fig:oscill} shows such a scenario.\\
	
	Furthermore, the Ricker Map exhibits another remarkable feature. Indeed, when $r$ exceeds $e^2$ there are no stable equilibrium consisting of a single value. After a transient period, population size starts oscillating among a fixed and finite number of distinct values. The set of these values is called the \emph{orbit}. These values are the fixed points of the equation $f^n(N_t) = N_t$ with $n \in \mathbb{N^*}$ and where $f^n = \underbrace{f\circ f\circ \cdots \circ f}_{n\text{\ times}}$. When $r=e^2$ the orbit consists of 2 values, then of 4 then of 8 and so on until a critical value above which solutions follow an aperiodic pattern. $e^2$ is called a \emph{bifurcation value}, and this geometric progression in the length of the cycles is called a \emph{period doubling cascade}. Figure~\ref{fig:stability} represents the orbit as $r$ grows ($K$ is fixed and equal to 1) and was obtained experimentally. It can be seen that when $r=e^2$ the orbit consists of 2 values and of 8 when $r=2e^2$. Figure~\ref{fig:oscill} shows such an orbit of four values.
	
	As $r$ continues growing, we rapidly reach a situation where population size does not enter any stable orbit any more. This leads to a behaviour characteristic of chaos, where a small change in the value of the parameters or the initial conditions leads to very different solutions. Figure~\ref{fig:chaos} shows the evolution of two populations when either parameter $r$ or initial conditions present a very minor change. It can be noted that populations sizes, in both cases, diverges rapidly from one another.
	
	\subsubsection{The Noisily Observed Ricker Map}
	\label{NRM}
	In order to allow for external and internal stochasticity~\cite{wang2007latent} and to take into account the observational process~\cite{calder2003incorporating} (for example a counting process of salmons in fisheries), extensions to the deterministic case have been suggested. We chose to proceed with the following model, suggested by Wood (2010)~\cite{Wood2010}.
	\begin{align}
	& N_t = r N_t e^{-N_t+Z_t} \hspace{1cm} Z_t \distas{\mathrm{iid}} \mathrm{N}(0, \sigma^2) \label{noisyRickerState}\\
	& Y_t = \mathrm{Poisson}(\phi N_t)
	\label{noisyRickerObservation}
	\end{align}
	Therefore, $N_t \sim \log\mathcal{N} (\log{(rN_{t-1}e^{-N_{t-1}})},\sigma^2)$, i.e $N_t$ is log-normally distributed with parameters depending on $r$, $K$ and $\sigma$. \\
	
	This models belongs to the framework of state space models which have been widely used to describe population dynamics~\cite{lillegaard2008estimation}~\cite{zhang2009spatial}~\cite{zhang2010computational} as it allows for great flexibility, encompassing models ranging from linear gaussian to highly non-linear and non-gaussian. 
	
	The statistical problem at hand is to estimate the joint probability of $(r, K, \sigma, \phi)$ in order to determine which of the regimes described earlier drives the observed population. Due to the chaotic dynamics of the Ricker map, estimating these coefficients with precision is important if one wants to obtain simulations exhibiting the same properties as experimental data. However, such an erratic behaviour leads to a highly multimodal likelihood and traditional approaches do not suit this parameter estimation problem. \\
	Moreover, even if the map were not chaotic, the likelihood is a highly dimensional integral over $N_{0:T}$ (for ease of notation we denote $N_{0:T} \coloneqq \{N_0, N_1, \cdots, N_T\}$). Indeed we have:
	\begin{equation}
		L(y_{1:T}; \phi, r, \sigma) = \int_{0}^{\infty}p(y_{1:T}, n_{0:T} | \phi, r, \sigma)\mathrm{d}n_{0:T}
	\end{equation}
	and $T$ is ranges typically from 30 to several hundreds. Therefore classic numerical integration tools, or Markov Chain Monte Carlo algorithms which require calculation, up to a constant, of the likelihood, are not suited for this problem. Therefore a wealth of techniques have been designed solve this problem: iterated filtering~\cite{ionides2006inference}, data cloning~\cite{lele2007data} and adaptative PMCMC~\cite{peters2010ecological}.
	
	\subsection{Nicholson's experiment on Sheep Blowflies} \label{(Nicholson)}
	Nicholson performed in 1954 and 1957 several
	laboratory experiments in order to better understand the population dynamics of \emph{Lucilia cuprina} (also known as sheep blowfly)
	under resource limitation. Blowflies development occurs in four stages the eggs giving larvae which evolve into pupae before becoming adults. Two experiments  (E1 and E2) consisted in restricting the amount of resources available to the larvae while adults had unrestricted access to sugar and water but a limited one to protein (which are necessary to produce eggs). Two others (E3 and E4) consisted in restricting respectively moderately and severely resources to larvae whereas adult were granted unlimited food. Gurney et al.~\cite{gurney1980nicholson}, 30 years later, suggested the following differential equation to model this delayed recruitment process:
	\begin{equation} \label{blow}
		\frac{d N(t)}{d t} = PN(t-\tau)e^{-\frac{N(t-\tau)}{N_0}} -  \delta N(t) = B(N(t-\tau))) - D(N(t))
	\end{equation} 
	where $N_t$ is the population size at generation $t$, $B$ is the birth rate function and involves the maturation delay $\tau$, i.e the time taken for a blowfly until it becomes an adult, and $D$ the death rate function which depends on the current population, which is in this case a simple linear function. Note that here the birth process is in fact a delayed version of the Ricker map studied in section~\ref{rickerGen} and therefore the interpretation of $P$ is the same as that of $r$. Usually, $P > \delta$ \\
	Following Berezantsky et al.~\cite{berezansky2010nicholson} we can derive several interesting properties. First the carrying capacity $K$ is the solution to $B(K)=D(k)$, is such that $K=N_0\log\frac{P}{\delta}$. An important characteristic of $K$, in this context, is that when $N_t < K$ $B(N_t) > D(N_t)$, i.e the population grows, and when $N_t > K$ the death rate $D$ is larger than the birth rate $B$, i.e the population decrease. It gives an interesting interpretation of the carrying capacity, which is that it is the maximum population size which can be attained in a certain environment.
	
	Wood~\cite{wood2010statistical} produced a stochastic version of Equation~\ref{blow} to introduce stochasticity in the dynamics of the blowfly population. Discretizing the original equation and adding demographic stochasticity to both death and birth processes and perturbing their rates with environmental noise led to the following model:
	\begin{align}
	& N_t = R_t + S_t \\
	& R_t \sim  \mathrm{Poisson}(PN_{t-\tau}e^{-\frac{N_{t-\tau}}{N_0}}u_t)  \hspace{1cm} u_t \sim \mathrm{Gamma}(\sigma_p^{-2}, \sigma_p^{-2})\\ 
	& S_t \sim \mathrm{Binomial}(e^{-\delta N_{t-1}}v_t) \hspace{2cm}  v_t \sim \mathrm{Gamma}(\sigma_d^{-2}, \sigma_d^{-2}) 
	\end{align}
	where $R_t$ is a Ricker map delayed recruitment process and $S_t$ a survival process which models the fact that each adult has
	an independent probability of $e^{-\delta N_{t-1}}$ of surviving at each time step. Environmental noise is modelled by $u_t$ and $v_t$ whose particular parametrisation enforces a unit mean and a variance equal to $\sigma_{p,d}^2$. Even though Nicholson's experiments were conducted in the controlled environment of a laboratory, Wood showed, using goodness of fit tests between real data and simulated observations, that removing environmental noise, that is setting $u_t=v_t=1$ led to poor fit across all experiments (E1 to E4). This is explained, according to Wood, by the fact that the data present irregular cycle which can be accounted for resorting only to internal stochasticity. Since the recruitment process is a delayed Ricker map, the same difficulties arise in parameter estimation.  

	\section{Inference Methods}
	\subsection{State Space Model}
	State space models describe sets of processes which can be decomposed into the following form where $\mathrm{X}=\{x_t ; t \in \mathbb{N}\}$ is a discrete time with $x_t \in \Omega_s$ and $\mathrm{Y}=\{y_t ; t \in \mathbb{N^*}\}$ is another discrete time process with $y_t \in \Omega_o$, $\Omega_s$, $\Omega_o$ being samples spaces. The $x_t$'s are unobserved and are called the \emph{hidden states} and the $y_t$ are the \emph{observations}. Moreover, $\theta$ is a vector of parameters on which both the distribution of $\mathrm{X}$ and $\mathrm{Y}$ depend. For the purposes of this dissertation, $\Omega_s, \Omega_o \subseteq \mathbb{R}$ and $\theta \in \mathbb{R}^k$, $k \in \mathbb{N^*}$. This can be summarized as follow: 
	\begin{align}
		& p(x_0, \theta) \\
		& p(x_t | x_{t-1}, \theta) \hspace{1cm} t \ge 1\\
		& p(y_t | x_t, \theta) \hspace{1cm} t \ge 1
	\end{align}
	Note that we denote by $p(x_t)$ both the probability density of $X_t$ and its distribution if it exists, with respect to an underlying measure $\lambda$. This set of equations means that $X_t$ is a Markov process of initial distribution $p(x_0)$ and transition distribution $p(x_t | x_{t-1})$ and that the observations $y_t$ are assumed to be independent conditionally on $\{x_t ; t \in \mathbb{N}\}$, i.e $p(y_1, \cdots, y_t | x_0, \cdots, x_t) =p(x_0)\Pi_{k=1}^t p(y_k | x_k)$ where $p(y_k | x_k)$ is the marginal distribution of $y_k$. We can therefore decompose the full joint density as follows:
	\begin{align}
	\underbrace{p(x_{0:T}, y_{0:T}, \theta)}_{\text{joint}} & = \underbrace{p(x_{0:T}, \theta)}_{\text{prior}}\underbrace{p(y_{1:T}| x_{0:}, \theta)}_{\text{likelihood}} \\
		& = p(\theta)p(x_0| \theta)\prod_{k=1}^{T}p(x_k|x_{k-1}, \theta)\prod_{k=1}^{T}p(y_k|x_k, \theta)
	\end{align}\\
	
	\subsubsection{Inference on State Space Models}
	Bayesian inference on state space models consists in obtaining, conditioned on a particular dataset $y_{1:T}$, the \emph{posterior distribution} $p(x_{0:T}, \theta| y_{1:T})$. This task is divided between \emph{parameter estimation}, i.e obtaining $p(\theta | y_{1:T})$  and \emph{state estimation}, i.e obtaining $p(x_{0:T}|y_{1:T}, \theta)$. \\
	Traditional methods used for bayesian inference, such as Metropolis-Hastings and Gibbs sampling are, in most of the cases, not usable as the posterior distribution and the transition density are not known distributions and even seldom have closed forms (which precludes computation of the Metropolis Hastings acceptance ratio and of the full conditional distributions in Metropolis Hastings within Gibbs). Examples of this situation abound~\cite{beskos2006exact}~\cite{fearnhead2008particle}~\cite{murray2011particle}. Even if these closed forms were available, states are usually strongly correlated and are also often correlated with model parameters. In this case, both Metropolis Hasting and Gibbs samplers are known to perform poorly~\cite{van2011partially}. \\

	
	\paragraph{Parameter estimation}
	In both of the population dynamic models presented earlier, the aim is to perform parameter estimation so as to understand which regime the population at hand is in, and to be able to carry out simulations. \\
	To achieve this the marginal Metropolis Hastings algorithm~\cite{hastings1970monte}, combined with methods to obtain samples from  $p(x_{0:t}| y_{1:t}, \theta)$ and to calculate unbiased estimates of the likelihood, is a method of choice. \\
	This algorithm samples in fact from $p(\theta, x_{0:T} | y_{1:T})$ and is a otherwise a simple Metropolis Hastings (MH) sampler. The proposal, which matches the structure of $p(x_{0:T}, \theta | y_{1:T})$ is of the form: $q((\theta^*, x_{0:T}^*) | (\theta, x_{0:T})) = q(\theta^* | \theta)p(x_{0:T}^* | y_{1:T}, \theta^*)$ and the acceptance ratio of the MH algorithm is: 
	\begin{align*}
	\frac{p(\theta^*, x_{0:T}^* | y_{1:T})q(\theta^ | \theta^*)p(x_{0:T} | y_{1:T}, \theta)}{p(\theta, x_{0:T} | y_{1:T})q(\theta^* | \theta)p(x_{0:T}^* | y_{1:T}, \theta^*)} & = \frac{p(\theta^* | y_{1:T})q(\theta | \theta^*)}{p(\theta | y_{1:T})q(\theta^* | \theta)} \\
	& = \frac{p(y_{1:T} | \theta^*)p(\theta^*)q(\theta | \theta^*)}{p(y_{1:T}|\theta)p(\theta)q(\theta^* | \theta)}
	\end{align*}
	
	It can be noted that the terminology comes from the fact that the ratio seems to be targeting $p(\theta | y_{1:T})$.
	
 	When the relationship between states and the one between states and observations are linear and gaussian, Kalman (1960)~\cite{Kalman1960} designed a method which allows to sequentially calculates the \emph{filtering distribution}, i.e $p(x_{t}| y_{1:t}, \theta)$, which in this particular case has a closed form. From there, it is easy to recover the likelihood marginalised over $x_{0:T}$ and to sample from $p(x_{0:t}| y_{1:t}, \theta)$. In this case the proposal $q(x_{0:t}| y_{1:t}, \theta)$ Although variants of this original filter, which deal with non-linear and non-gaussian state space miodels, have been designed, such as the extended~\cite{McElhoe1966} and unscented~\cite{Julier1997} Kalman filters, they give biased estimates of $p(x_t|y_{1:t}, \theta)$. \\
 	
 	In these non-linear, non-gaussian cases, the \emph{particle filter}~\cite{Gordon1993}, which is of the family of Sequential Monte Carlo (SMC) methods, provides estimates of $p(y_{1:T}, \theta)$ and of $p(x_{0:T}|y_{1:T}, \theta)$ which, when used as the proposal distribution, leaves invariant $p(x_{0:T}, \theta|y_{1:T})$ and ensures that the marginal Metropolis Hastings sampler is ergodic. This version of the algorithm is commonly referred to as the Particle Marginal Metropolis Hastings (PMMH) sampler and is described, along with the proof that it is indeed ergodic under mild assumptions in Andrieu et al.~\cite{andrieu2010particle}.
	
	\subsubsection{Particle Filter}
	We describe a little further the particle filter as it is the method we applied in order to estimate the likelihood and sample from $p(x_{0:T}|y_{1:T}, \theta)$ to carry out parameter inference in the case of the noisily observed Ricker map. \\
	This method is based on sequential importance sampling.
	The idea behind importance sampling is to approximate $p(x_{1:t}|y_{1:t}, \theta)$ with the empirical distribution
	\begin{equation}
	\hat{p}(x_{0:t}|y_{1:t}, \theta) = \sum_{i=1}^{N}\tilde{w}_t^{(i)} \delta_{x_{0:t}^{(i)}}(x_{0:t})
	\end{equation}
	where $\tilde{w}_t^{(i)} = \frac{W(x_{0:t}^{(i)})}{\sum_{j=1}^{N} W(x_{0:t}^{(j)})}$ and $W(x_{0:t}) = \frac{p(x_{1:t},y_{1:t}, \theta)}{q(x_{1:t})}$ where $x_{0:t}^{(i)}$ are iid draws from $q(x_{1:t})$, which is an easy to sample from distribution.
	
	However, this method does not take advantage of dependency structure of the state space model. Indeed, the weights $W(x_{0:t})$ can be expressed sequentially using the fact that the $y_t$ are conditionally independent given $x_{0:t}$ and that the $x_t$ have the Markov property. Moreover a choice of proposal distribution which also has the Markov property (i.e $q(x_{1:t})=q(x_{1:t-1})q(x_t| x_{t-1})$) allows us to write:
	\begin{align}
		W(x_{0:t}) & = \frac{p(x_{0:t},y_{1:t}, \theta)}{q(x_{0:t},y_{1:t}, \theta)} \\
		& = p(y_t|x_t)p(x_t|x_{t-1})
	\end{align}
	
	If we take any bounded measurable function $\phi$ Geweke (1989)~\cite{Geweke1989} showed, under certain conditions, that \begin{equation*}
		 \mathrm{I_N^{IS}}=\mathbb{E_{\hat{p}}}\phi(x_{0:t}) = \sum_{i=1}^{N} \tilde{w}_t^{(i)} \phi(x_{0:t}^{(i)}) \xrightarrow{\mathrm{a.s}} \int_{\Omega^n} \phi({x_{0:t}})p(x_{0:t}|y_{1:t})\mathrm{d}x_{0:t}=\mathbb{E_{{p}}}\phi(x_{0:t})
	\end{equation*}
	and moreover that the bias and the variance are $\mathcal{O}(\frac{1}{N})$.\\
	
	However, as shown in ~\cite{kong1994sequential}, the variance of the weights is exponential in the number $n$ of time steps. Moreover, the effective sample size (ESS), as defined in ~\cite{liu2008monte} pp 35-36, which measures the ratio between the variance of $\mathrm{I_N^{IS}}$ and of the same quantity if the samples were independently drawn from the true distribution, vanishes quickly.  
	
	The idea introduced by particle filters is to introduce resample moves. This consists in sampling, at each time step, from the importance sampling approximation $\hat{p(x_{0:t}|y_{1:t}, \theta)}$. This is achieved by selecting $x_{0:t}^{(i)}$ with probability $\tilde{w_n}^{(i)}$. Since sequential importance sampling propagates $N$ particles, this resample move is performed $N$ times at each time step. If $N_n^{(i)}$ is the number of offspring of each particle $x_{0:t}^{(i)}$, the final approximation of the target obtained is 
	\begin{equation*}
	 	\hat{p}(x_{0:t}|y_{1:t}, \theta) = \sum_{i=1}^{N}\frac{N_t^{(i)}}{N} \delta_{x_{0:t}^{(i)}}(x_{0:t})
	\end{equation*}
	Resampling has the beneficial effect to make the weights more balanced by removing samples far from regions of high density of the target distribution. Moreover, at each time step, the propagated particles are drawn from $\hat{p}(x_{0:t}|y_{1:t}, \theta)$ which leads to approximate the target by a series of closer and closer targets, from $\hat{p}(x_0|\theta)$ to $\hat{p}(x_{0:T}|y_{1:T}, \theta)$.
	 
	Using the fact that $p(y_{1:T}| \theta) = p(y_1|\theta)\prod_{k=2}^{T}p(y_k|y_{1:k-1}, \theta)$ and that \\
	$p(y_k|y_{1:k-1}, \theta) = \frac{1}{N}\sum_{i=1}^{N}W_k^{(i)}$ we have $\hat{p}(y_{1:T}| \theta)=\frac{1}{N}\prod_{k=1}^{T}\sum_{i=1}^{N}W_k^{(i)}$. It can be shown (see~\cite{del2004feynman}) that $\hat{p}(y_{1:T}| \theta$ is an unbiased estimate of the likelihood and we saw earlier that the particle filter allows to draw from an estimate (converging in distribution to) of $p(x_{0:T}|y_{1:T}, \theta)$, which are the quantities needed to make us of the PMMH sampler. For more details on SMC methods and the particle filter see~\cite{doucet2009tutorial} and for more details on resampling and its complexity see~\cite{murray2013parallel}.
	
	\section{Inference on the noisily observed Ricker Map}
	\subsection{Algorithm}
	In order to perform inference on the noisily observed Ricker Map, we used a PMMH sampler along with particle filter. \\
	We restate here the state space model, given in~\ref{NRM}, which is used to describe this population dynamics model.
	\begin{align}
	& N_t = r N_t e^{-N_t+Z_t} \hspace{1cm} Z_t \distas{\mathrm{iid}} \mathrm{N}(0, \sigma^2)\\
	& Y_t = \mathrm{Poisson}(\phi N_t)
	\end{align}
	
	\subsubsection{Particle Filter} \label{pfRIcker}
	Theoretical results indicate that the proposal density to use in order to minimize the variance of the likelihood estimate is $p(n_t | y_t, n_{t-1})$. However we have $p(n_t | y_t, n_{t-1}) \propto p(y_t|n_t)p(n_t|n_{t-1})$ where $p(y_t|n_t)$ is a Poisson distribution and $p(n_t|n_{t-1})$ a log normal distribution. This does not yield a known distribution from which we could sample. However, the conjugate distribution of a Poisson is a Gamma distribution. Therefore approximating the transition density with a Gamma distribution yields a Gamma proposal which is, hopefully, close enough from the true optimal proposal distribution. \\
	
	In order to approximate a Lognormal distribution with the closest Gamma distribution, we chose to minimize the Kullback-Liebler (KL) divergence~\cite{kullback1951information} between these two distributions. Indeed, the KL divergence measure the information for discrimination between two hypothesis regarding the population from which as sample is drawn. In a nutshell it means that the smaller is this divergence, the closer two probability measures are. Here we aim to minimize:
	\begin{equation}
	D_{KL}(P||Q)(\alpha, \beta) = \int_{0}^{\infty}{p(z|\mu, \sigma^2)\log(\frac{p(z|\mu, \sigma^2)}{q(z|\alpha, \beta)})\mathrm{d}z}
	\end{equation}
	where $p$ is the probability density function of a $\log\mathcal{N}(\mu, \sigma^2)$ and $q$ of a Gamma with shape $\alpha$ and scale $\beta$. Minimization finding critical points of the divergence give, $\alpha =\frac{1}{\sigma^2}$ and $\beta=\frac{1}{\alpha}e^{\mu+\frac{\sigma^2}{2}}$, and in the specific case of our state space model $\alpha(n_{t-1})= \frac{1}{\sigma^2}$ and $\beta(n_{t-1})=\sigma^2e^{\log(rn_{t-1}e^{-n_{t-1}})+\frac{\sigma^2}{2}}$. See~\ref{KLRicker} for details. \\
	We thus approximate the transition density at each time step $t$ with:
	\begin{equation*}
	q(n_t|\alpha(n_{t-1}), \beta(n_{t-1}), \theta) = \mathrm{Gamma}(\ \cdot \ ; \alpha(n_{t-1}), \beta(n_{t-1}) )
	\end{equation*}
	This leads us to the following proposal for the particle filter:
	\begin{equation*}
	\begin{split}
	q(n_t|n_{t-1}, y_t, \theta) & \propto  p(y_t|n_t, \theta)q(n_t|n_{t-1}, \theta) \\
	& \propto e^{-\phi n_t}(\phi n_t)^{y_t}n_t^{\alpha(n_{t-1})-1}e^{-\frac{n_t}{\theta(n_{t-1})}}
	\end{split}
	\end{equation*}
	i.e:
	\begin{equation*}
	q(n_t|n_{t-1}, y_t, \theta) = \mathrm{Gamma}(\ \cdot \ ; y_t+\alpha(n_{t-1}), \frac{\beta(n_{t-1})}{\beta(n_{t-1})\phi + 1})\end{equation*} \\
	
	We chose to use a multinomial resampler in our particle filter and chose to resample at each time step. The pseudo-code of the algorithm we used for the particle filter is given in Algorithm~\ref{pf}. It returns samples from $p(x_t | y_{1:t}, \theta)$, estimates of the log-likelihood $\hat{l}_t$ and the effective sample size $\mathrm{ESS}_t$ at each time step. \\
	This algorithm was implemented in Python, using Numpy/Scipy to simulate from known random variables and Cython to speed up critical code~\cite{wilbers2009using, behnel2011cython} (such as density calculations). Simulation showed that Numpy implementation of multinomial and gamma sampling were linear in the number of particle $N$. The rests of the calculations (weight, ESS, likelihood, parameters of the proposal distribution) are also linear in the number of particle. As for space complexity, one estimate of the filtering distribution is stored and each time step, along with the likelihood estimate and the ESS. Therefore the space complexity is linear in the number of steps $T$. Experimental simulations, shown in Figure~\ref{runningRicker}, for a number of particle varying from 10 to 10000, of the running time of Algorithm~\ref{pf} averaged over 100 repetitions, confirms an average running time of $\mathcal{O}(N)$.
	
	\subsubsection{Particle Marginal Metropolis Hastings Sampler}
	Algorithm~\ref{pmmh} shows the straightforward implementation of the PMMH sampler we used. At each time step a new set of parameter is proposed using a multivariate random walk whose components are independent. We chose independence as we did not have any information on the correlation structure of the vector of parameters. Following Fasiolo~\cite{fasiolo2014statistical} we also chose independent uniform priors on the parameters. \\
	
	After a burnin period, we adapt the covariance matrix of the proposal using the Robbins-Monro update, as described in Andrieu and Thoms~\cite{Andrieu2008}. Indeed, as proved by Roberts et al.~\cite{roberts1997weak} for a certain class of target distribution (namely those of the form $\pi_n(x) = \prod_{i=1}^{n}f(x_i)$ where $x \in \mathbb{R}^n$ and certain regularity conditions on f, the same result for other form of $\pi$ has been proved, see~\cite{roberts2001optimal}), the optimal acceptance rate, when using a multivariate random walk proposal, in term of mixing, is close to 0.234. In the case of a PMMH sampler, where the likelihood is an approximation of the true one, this rate should be decreased to 0.1~0.15 (as suggested by Dr. Lawrence Murray). This adaptation period allows us not to worry too much about the initialisation of the covariance matrix of the random walk. When adapting during the whole chain, care should be exercised regarding the convergence towards the target distribution of the MCMC algorithm. However, we chose to adapt only for a brief (although long enough to reach the targeted acceptance rate) period of time, typically 10 to 15\% of the total length of the chain.\\
	
	More specifically the Robins-Monro update is used to rescale the covariance matrix of the random walk. In the case of a univariate proposal $q_\theta \sim \mathcal{N}(0, exp(\theta))$ and target distribution $\pi$,  what adaptation aims to do is to find the zeros of $h(\theta) = \mathbb{E}_{\pi\otimes q_\theta}\mathrm{H}(\theta, x, y)$ where $\mathrm{H}(\theta, x, y) = \min(1, \frac{\pi(y)}{\pi(x)}) - \alpha^*$ and $\alpha^*$ is the target average acceptance rate. Robins-Monro update proceed iteratively by setting $\theta_{i+1} = \theta_i + \frac{1}{i+1}(\hat{\alpha}_{\theta_i} - \alpha^*)$ where $\hat{\alpha}_{\theta_i} = \frac{1}{L}\sum_{k=1}^{L}\frac{\pi(y_{iL+k})}{\pi(x_{iL+k-1})}$. If $\hat{\alpha}_{\theta_i}$ is unbiased, Robbins and Monro~\cite{robbins1951stochastic} have shown that, under certain regularity conditions on H and and $\hat{\alpha}_\theta$, $\theta_i$ converges in probability to $\alpha^*$. Here, it can be simply understood noticing that if the acceptance rate $\hat{\alpha}_\theta$ is greater than the target $\theta$ and thus the variance of the random walk is increased, leading to greater moves in the parameter space and thus a reduced acceptance rate. The contrary happens when $\hat{\alpha}_\theta$ is smaller than the target. Principled statistical rules exists to determine when to stop the adaptation phase, but, since we were targeting a range rather than a specific value for the acceptance rate,  we relied on trial and error to determine its approximate duration to reach the range 0.1-0.15. Finally, since we used a multivariate random walk with a diagonal covariance matrix, we decided to simultaneously adapt its coefficient using the Robbins-Monro update. This very simple approach proved satisfactory as all the chains we ran attained average acceptance rate within the targeted range.
	
	
	\subsection{Synthetic Data}
	In order to obtain a first assessment of the efficiency of our PMMH algorithm, and to compare our results to Fasiolo's~\cite{fasiolo2014statistical}, we used simulated datasets from the Ricker map model. It should be noted that Fasiolo use a slightly different version of the more general one we gave, where $K$ is set to one and is not allowed to vary. However, even though we shall conform with this definition in this section, when we use real data later on, $K$ will be again allowed to vary. As in Fasiolo's, we chose to set $r=log(3.8)$, $\sigma=0.3$, $\phi=10$ and $T=50$ (such a short path is chosen because real data on population size is seldom longer). This places us in the  chaotic regime of the Ricker map as $log(3.8) \approx 44.7 > e^2$. \\
	
	We first compared the evolution of the effective sample size using our proposal (as described in Section~\ref{pfRIcker}) and the transition density $p(n_t|n_{t-1})$ as proposal, called \emph{prior proposal} (this version of particle filter is called \emph{bootstrap filter}) used by Fasiolo. Figure~\ref{fig:essRicker} shows this comparison. Note that we used 500 particles in each cases. It can be seen that the use of the prior proposal leads to an extremely irregular ESS, with very sharp dips, indicative of very imbalance weights. This is due to the fact that the transition density approximates badly the successive targeted distributions. On the contrary, our gamma approximation to the optimal proposal, because it incorporates knowledge from the actual observations, is less irregular, and shows very few dips below $\frac{N}{2}$. \\
	
	We also compared the stability of the maximum likelihood estimate for $r$ (keeping $\sigma$ and $\phi$ fixed and equal to their true value) when using the prior proposal and our gamma approximation to the optimal proposal. We calculated the likelihood for $r \in [12, 90]$ using a discretisation size equal to 0.5. We repeated the experiment 50 times. The variance of the maximum likelihood estimates and the mean squared errors obtained using different numbers of particles are displayed in Table~\ref{table:mleR} where the index 1 denotes the prior proposal and index 2 our gamma approximation. Figure~\ref{fig:comparisonR}, shows the MLE obtained across the iterations using 50, 100, 200, 500, 1000 and 1500 particles. \\
	We used the same methodology for $\phi$ and $\sigma$, using respectively a discretisation path of 0.06 and 0.003 in the intervals $[5, 15]$ and $[0.15, 0.6]$. Table~\ref{fig:mlePhi}, Table~\ref{fig:mleSigma}, Figure~\ref{fig:comparisonPhi} and Figure~\ref{fig:comparisonSigma} respectively show the evolution of the variance and the mean square error of the maximum likelihood estimates and the MLE obtained across the iterations. Finally, Figure~\ref{fig:transect} shows transect of the log-likelihood with respect to $r$, $\phi$ and $\sigma$. 
	
	We performed parameter inference on this synthetic dataset using our implementation of a PMMH sampler and our gamma approximation for the particle filter responsible for the likelihood estimation. First, we sampled five chains from the same dataset, using initial values for the parameters sampled from normal distributions,  in order to perform the usual convergence and mixing diagnostic on an MCMC algorithm. Mixing is assessed via graphical inspection of the samples (traceplot). Figure~\ref{fig:traceplotDiag} shows the traceplots for each of the parameters. The parameter space seem to be satisfactorily explored and no strong correlation pattern can be seen. This is further confirmed by inspecting the autocorrelation function (ACF) of the samples, displayed in Figure~\ref{fig:acfDiag}. For each parameter the ACF decreases quickly under the significance threshold. Convergence is assessed using running means and the Gelman-Rubin diagnostic~\cite{gelman1992inference}. We denote $m_i$ such that $m_i=\frac{j=1}{i}\sum_{j=1}^{i}s_j$, where $s_i$ is the i\textsuperscript{th} sample of a chain, as the running mean of a chain. It can be seen in Figure~\ref{fig:rmDiag} that the running means converge to the same values for each parameters across the chain regardless of the initial values. This is a good indication that the samples obtained from the PMMH sampler are distributed according to the same stationary distribution. However, it is also visible that the posterior estimates are biased (for example, the posterior mean for $r$ is just below 40, whereas its true value is 44.7). Even though the likelihood estimate given by the particle filter is unbiased, it is a logic consequence of using a approximation instead of the true likelihood. \\
	The Gelman-Rubin diagnostic plots the values of $R = \sqrt{\frac{\mathrm{Var}(\alpha)}{W}}$ where $W$ is the average of the within-chain variances, where the within-chain variance is the empirical variance of the chain, and $ \mathrm{Var}(\alpha) = \frac{n-1}{n}W + \frac{1}{n}B$ where $B$ is the between-chain variance, \emph{i.e} the empirical variance of the means of the chains. $R$ should converge to one as the length of the chains increases, if, regardless of the initialization, the chains converges to the same stationary distribution. It can be seen in Figure~\ref{fig:gelmanDiag} that $R$ converges towards 1 quickly for all parameters. \\
	
	However, the aim was again to compare our results to those presented by Fasiolo in order to see the improvement brought by our gamma approximation. Following his algorithm we used uniform priors on the parameters, each time setting the lower bound to half the true value of the parameter and the upper bound to 1.5 that of the true value. We discarded, as Fasiolo, 2500 samples as a burnin, and set, unlike Fasiolo this time, 2500 iterations as an adaptation phase. We set the total length of the chain to 17500 iteration so as to have the same number of samples from which to estimate the posterior distribution of the parameters as Fasiolo, that is to say 12500 iterations. In order to compare our results, we used the same two metrics as defined by Fasiolo. These metrics, calculated based on the log of the parameters, are the median squared errors for each parameters and the median and inter-quartile range of the squared errors, averaged geometrically across the parameters, that is to say of the $e_j = ((\log\hat{ r}_j-\log r)(\log\hat{\sigma}_j-\log\sigma)(\log\hat{\phi}_j-\log\phi))^\frac{1}{3}$ where $j$ denotes the number of the experiment and a hat denotes a posterior mean. This use of logarithms is not specified or explained in Fasiolo's paper, but we followed this usage in order to be able to compare our results to his. We also performed inference using LibBi which is a software package for state-space modelling and Bayesian inference~\cite{murray2013bayesian}. This well established software, which implements a PMMH sampler, which uses a bootstrap filter, among other samplers, was used to represent a reliable baseline to which we compared both our and Fasiolo's results. Fasiolo used 250 synthetic dataset to calculate his metrics, whereas we used only 50 due to time restrictions. \\
	Table~\ref{table:mse} reports the median squared errors obtained whereas Table~\ref{table:metric} reports Fasiolo's custom metric. It can be seen that, apart for $\sigma$ where LibBi and our PMMH sampler give MSE an order of magnitude lower, the MSE are of the same order of magnitude across all methods. Similarly, Fasiolo's custom metric are of the same order of magnitude across all methods. \\
	
	Having seen that our method is comparable in term of the two metrics used by Fasiolo, and that it yields more stable maximum likelihood estimates, with lower mean squared errors for small number of particles, the significant improvement come from the autocorrelation of the samples. Indeed, our method is only 6.5\% more computationally expensive, as measured on particles filter with a number of particles ranging from 10 to 1000, averaged over a hundred runs and for the same computational power, we found that, on average, autocorrelation of the samples from $r$ was 13\% lower, that for $\phi$ 10\% lower and that for $\sigma$ another 13\% lower. (autocorrelation was measured using the effective sample size of each chain). These values were obtained by running our PMMH sampler on 50 different datasets, and for each dataset using once a bootstrap filter and once our particle filter.
	
	\subsection{Global Population Dynamics Database Data}
	The Global Population Dynamics Database (GPDD) is a joint effort by the British institution the NERC Centre for Population Biology, which was hosted by the Imperial College, and two American institutions, the National Center for Ecological Analysis and Synthesis and the Department of Ecology and Evolution, University of Tennessee. It is the largest collection of animal and plant population data in the world, which brings together close to five thousand time series. We chose the same datasets as Gao et al.~\cite{gao2012bayesian}. These datasets are chosen so that they represent all the different regimes of the Ricker map. There are therefore five categories of population size dynamics: increasing (CAT1), decreasing (CAT2), quasi-periodic with small variations (CAT3), quasi-periodic with large variations (CAT4) and irregular with outbreaks (CAT5). It is difficult to compare our results to those obtained by Gao et al. as they used a different state space model to fit the data. Indeed, if they used the same transition density for the space, they observational process is based on log transformed observations and is a simple normal distribution with mean equal to the log of the state (this model is however broadly used, see~\cite{de2002fitting, peters2010ecological, valpine2005state}). Nonetheless, we can use states estimates given by our particle filter using the posterior means of the parameters estimates given by the PMMH sampler to form an idea of how good these estimates are. \\
	
	We further describe one particular example among all the datasets on which inference was performed. This dataset comes from CAT4, a category on which Gao et al. sampler does not perform well. It describes the evolution of a population of \emph{Lagopus lagopus scoticus} (a Scottish bird) during 76 years every year, from 1866 to 1942. \\
	We parametrised the PMMH sampler using the following priors: $r \sim \mathrm{U}([1, 60])$, $\phi \sim \mathrm{U}([1, 30])$, $\sigma \sim \mathrm{U}([0.1, 0.6])$, $K \sim \mathrm{U}([1, 10000])$. The range for $r$ covers all the regimes of the Ricker map, the one for $\phi$ allows a large enough multiplication effect between states and observations, and finally the one for $K$ accommodates all the type of population in the datasets we analysed. We set a burnin period of 2500 iterations and an adaptation period of the same length, with as previously a target acceptance rate between 0.15 and 0.1, and sampled during 12500 afterwards. We obtained posterior parameters which, once given as input to our particle filter, give simulated observations close the actual data. This example serves also to demonstrate that our PMMH sampler mixes well on real data and converges to a stationary distribution as shown respectively by the autocorrelation function (Figure~\ref{acfLagopus}), the running means (Figure~\ref{rmLagopus}) and the Gelman-Rubin factor (Figure~\ref{gelmanLagopus})
	
	\section{Inference on Nicholson's experiments}
	\subsection{Algorithm}
		Once again, we used a PMMH sampler along with a tailored particle filter to perform bayesian inference on the model described in section~\ref{Nicholson}. We restate here the model to make it easier to follow the mathematical developments:
		\begin{align*}
		& N_t = R_t + S_t \\
		& R_t \sim  \mathrm{Poisson}(PN_{t-\tau}e^{-\frac{N_{t-\tau}}{N_0}}u_t)  \hspace{1cm} u_t \sim \mathrm{Gamma}(\sigma_p^{-2}, \sigma_p^{-2})\\ 
		& S_t \sim \mathrm{Binomial}(e^{-\delta N_{t-1}}v_t) \hspace{2cm}  v_t \sim \mathrm{Gamma}(\sigma_d^{-2}, \sigma_d^{-2}) 
		\end{align*}
	\subsubsection{Particle Filter}
	Nicholson's blowfly model is not part of the state space model framework. Indeed, there is no observational process here and therefore no further uncertainty attached to it. However the global methodology of particle filter can still be applied. \\
	Here we are only interested in an unbiased estimate of the likelihood $p(n_1:t|\theta)$, instead of targeting $p(n_{0:t}|y_{1:t}, \theta)$ in order to be able to use this unbiased estimate in the PMMH sampler described in Algorithm~\ref{pmmh}. \\
	Once again, writing $p(n_{1:t}|\theta) = p(n_1|\theta)\prod_{k=2}^{t}p(n_k|n_{1:k-1}, \theta)$ as in the particle filter algorithm we want to find an unbiased estimate of $p(n_k|n_{1:k-1}, \theta)$. We can write that:
	\begin{align*}
	p(n_k|n_{1:k-1}, \theta) & = \int_{0}^{\infty}p(n_k, s_k |n_{1:k-1}, \theta)\mathrm{d}s_k \\
	& = \int_{0}^{\infty}p(n_k |s_k, n_{1:k-1}, \theta)p(s_k|n_{1:k-1}, \theta)\mathrm{d}s_k \\
	& = \int_{0}^{\infty}p(n_k |s_k, n_{1:k-1}, \theta)p(s_k|n_{k-\tau}, \theta)\mathrm{d}s_k
	\end{align*}
	since $S_k$ depends only on $n_{t-\tau}$.
	Therefore, sampling $N$ times $S_k$ from a well chosen (i.e close to $p(s_k|n_{k-\tau}, \theta)$) importance distribution $q(s_k|n_{k-\tau}, n_k, \theta)$ leads to the following unbiased estimate $\hat{p}(n_k|n_{1:k-1}, \theta) = \frac{1}{N}\sum_{i=1}^{N}w_k^i(s_k^i)$ where $w_k^i =p(n_k |s_k^i, n_{1:k-1}, \theta)\frac{p(s_k^i|n_{1:k-1}, \theta)}{q(s_k^i|n_{k-\tau}, n_k, \theta)}$. \\
	However, $p(n_k |s_k, n_{1:k-1}, \theta)$ is intractable in Nicholson's blowfly experiment model. Nonetheless we can applied the same technique as above a second time:
	\begin{align*}
	p(n_k |s_k, n_{1:k-1}, \theta) & = \int_{0}^{\infty}p(n_k, r_k|s_k, n_{1:k-1}, \theta)\mathrm{d}r_k \\
	& = \int_{0}^{\infty}p(n_k| r_k, s_k, n_{1:k-1}, \theta)p( r_k|n_{1:k-1}, s_k \theta)\mathrm{d}r_k \\
	& = \int_{0}^{\infty}\delta(n_k = r_k + s_k)p(r_k|n_{k-1} \theta)\mathrm{d}r_k
	\end{align*}
	since $p(n_k| r_k, s_k, n_{1:k-1}, \theta)=p(n_k| r_k, s_k)$ which is equal to 1 if $n_k = r_k + s_k$ and 0 otherwise (denoted by the use of Kronecker's delta symbol $\delta_x(y) = 1$ if $x=y$ and 0 otherwise) and $r_k$ conditional on $n_{k-1}$ is independent of $s_k$ and $n_{1:k-2}$. \\
	Therefore, if we sample $M$ times $R_k$ from another well chose importance distribution, we obtain the following unbiased estimate of $p(n_k |s_k, n_{1:k-1}, \theta)$: $\hat{p}(n_k |s_k, n_{1:k-1}, \theta)=\frac{1}{M}\sum_{j=1}^{M}\tilde{w}_k^j(r_k^j)(s_k)$ where $\tilde{w}_k^j(s_k) =\delta(n_k = r_k^j + s_k)\frac{p(r_k^j|n_{k-1} \theta)}{q(r_k^j|n_{k-1}, n_k, \theta)}$. \\
	Finally using iterated conditional expectation, it is easy to see that, combining the two previous unbiased estimates, we obtain another unbiased estimate of $p(n_k|n_{1:k-1},\theta)$ which is:
	\begin{equation}
	\hat{p}(n_k|n_{1:k-1},\theta) = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{M}\sum_{j=1}^{M}\tilde{w}_k^j(s_k^i)\frac{p(s_k^i|n_{1:k-1}, \theta)}{q(s_k^i|n_{k-\tau}, n_k, \theta)}
	\end{equation}
	This method is called a \emph{random weights particle filter} and was first described in a contribution by Rousset and Doucet to the article by Beskos et al.~\cite{beskos2006exact} where the use of random weights was first introduced for discretely observed diffusion processes.\\
	To summarise, the particle filter we used is described in Algorithm~\ref{RWPF}. Its time complexity is $\mathcal{O}(MN)$, to contrast to the particle filter used in Algorithm~\ref{pmmh}, whereas its space complexity remains $\mathcal{O}(N)$. \\
	
	There remain to find closed for equations for $p_s(s_k|n_{k-\tau}, \theta)$ and $p_r(r_k|n_{k-1}, \theta)$ and to design importance proposal for both $r_k$ and $s_k$. Both $r_k$ and $s_k$ densities, conditional on $n_{1:{k-1}}$ are compounded distributions. $r_k$ density is a Poisson distribution with a Gamma distributed parameter. Since a Gamma distribution is the conjugate prior of a Poisson, the resulting distribution has a closed form and is know as Negative-Binomial. It is a discrete distribution, whose support is $\mathbb{R}$, parametrised by two coefficients $r$ and $p$, where $r$ can be seen as the number of failures, and $p$ the probability of success. $P(X=k)$, where $X\sim \operatorname{NB}(r, p)$, is thus the probability that $k$ successes occur before the r\textsuperscript{th} failure. Its density can be expressed as: 
	\begin{equation*}
	P(X=k) = \binom{r+k-1}{k}p^k(1-p)^r
	\end{equation*}
	Applied to our case it yields $p_r(r_k|n_{t-\tau}, \theta) = \operatorname{NB}(\alpha, \frac{\beta(n_{k-\tau})}{\beta(n_{k-\tau}) + \alpha})$ where  $\beta(n_{k-\tau}) = Pn_{k-\tau}e^{-\frac{n_{k-\tau}}{N_0}}$ and $\alpha = \sigma_p^{-2}$. Derivation can be found in Appendix~\cref{rDensity}.
	As for $q_r(\ \cdot \ ; n_{k-\tau}, n_k)$ it is just the afore mentioned Negative-Binomial restricted to the interval $[0, n_t]$. \\
	
	The distribution of $s_k$ conditional on $n_{k-1}$ and $\theta$ is a Binomial whose probability of success is Gamma distributed. This compounded distribution does not yield any known distributions. However, using derivations presented in Appendix~\cref{sDensity}, a closed for can be found for $p(s_k|n_{k-1}, \theta)$ which is:
	\begin{equation*}
	p(s_k|n_{k-1}, \theta) = \binom{n_{k-1}}{s_k}\alpha^\alpha\sum_{l=0}^{n_{k-1}-s_k}\binom{n_{k-1}-s_k}{l}\frac{(-1)^l}{(\delta(s_k+l)+\alpha)^\alpha}
	\end{equation*}
	where $\alpha = \sigma_d^{-2}$
	
	
	\begin{algorithm}
		\caption{Particle filter}\label{pf}
		\begin{algorithmic}[1]
			\Function{PARTICLE-FILTER}{N, T, $\theta$}
			\BState t=0: // initialization
			\ForAll{$i \in \{1, \cdots, N\}$} 
			\State Sample $N_0^{(i)} \sim q_0(n_0|\theta)$
			\State Set $w_0^{(i)} \gets \frac{1}{N}$
			\State Set $\hat{l_0} \gets 0$ // initialize log-likelihood
			\State Set $\mathrm{ESS}_0 \gets N$ // initialize ESS
			\State Set $\hat{n}_0 \gets n_0$ // initialize state
			\State Set $t \gets 1$
			\EndFor
			\item[]
			\BState $1 \le t \le T$:
			\While{$t \le T$}
			\State Sample $(a_t^{(1)}, \cdots, a_t^{(N)} \sim \mathrm{Multinomial}(w_{t-1}^{(1)}, \cdots, w_{t-1}^{(N)})$ // ancestors 
			\ForAll{$i \in \{1, \cdots, N\}$}
			\State Set $\alpha, \beta \gets \text{CALC-PARAM}(n_{t-1}, \theta)$ // proposal parameters
			\item[]
			\State Sample $n_t^{(i)} \sim q(n_t| n_{t-1}^{a_t^{(i)}}, y_t, \theta)$ // propagate particle
			\item[]
			\State Set $w_t^{(i)} \gets \frac{\strut p(n_t|n_{t-1}^{a_t^{(i)}}, \theta)}{\strut p(y_t|n_t^{(i)}, \theta)q(n_t|n_{t-1}^{a_t^{(i)}, \theta}, y_t)}$ // weight particle
			\EndFor
			\item[]
			\State $\hat{l}_t \gets \hat{l}_{t-1} - \log N + \log\sum_{i=1}^{N}w_t^{(i)}$
			\State Normalize $(w_t^{(0)}, \cdots, w_t^{(N)})$
			\State Set $\mathrm{ESS_t} \gets (\sum_{i=1}^{N}(\mathrm{W}_t^{(i)})^2)^{-1}$ // W denotes a normalized weight
			\State Set $\hat{n}_t \gets \sum_{i=1}^{N}\mathrm{W}_t^{(i)}n_t^{(i)}$
			\EndWhile
			\item[]
			\Return $\hat{l}_T$, $(\hat{n}_0, \cdots, \hat{n}_T)$,  $(\mathrm{ESS}_0, \cdots, \mathrm{ESS}_T)$
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Particle Marginal Metropolis Hastings Sampler}\label{pmmh}
		\begin{algorithmic}[1]
			\Function{PMMH}{N, T, $\theta_0$, filter, burnin, adaptation, samples, L}
			\State $l_0 \gets \text{filter}(N, T, \theta_0)$
			\ForAll{$i \in \{1, \cdots, \text{burnin}\}$}
			\State $\theta_i, l_i, \alpha_i \gets \text{ROUTINE}(N, T, \theta_{i-1}, l_{i-1}, \text{filter})$
			\EndFor
			\ForAll{$i \in \{\text{burnin}, \cdots, \text{adaptation}\}$}
			\State $\hat{\alpha}_\Sigma \gets 0$
			\ForAll{$j \in \{1, \cdots, L\}$}
			\State $\theta_i, l_i, \alpha_i \gets \text{ROUTINE}(N, T, \theta_{i-1}, l_{i-1}, \text{filter})$
			\State $\hat{\alpha}_\Sigma \gets \hat{\alpha}_\Sigma + \frac{1}{L}\alpha_i$
			\EndFor
			\State $\Sigma \gets \text{RESCALE}(\hat{\alpha}_\Sigma, \Sigma)$
			\EndFor
			\ForAll{$i \in \{\text{adaptation}, \cdots, \text{samples}\}$}
			\State $\theta_i, l_i, \alpha_i \gets \text{ROUTINE}(N, T, \theta_{i-1}, l_{i-1}, \text{filter})$
			\EndFor
			\Return $(\theta_{\text{adaptation}},\theta_{\text{adaptation}+1}, \cdots, \theta_{\text{samples}})$
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Routine for PMMH Sampler}\label{routine}
		\begin{algorithmic}[1]
			\Function{ROUTINE}{N, T, $\theta$, $l$, filter}
			\State $\theta^* \gets q_{RW}(\theta, \Sigma)$
			\State $l^* \gets \text{filter}(N, T, \theta^*)$
			\State $\alpha(\theta, \theta^*) = l^*+\log q(\theta|\theta^*) + \log p(\theta) - l - \log q(\theta^*|\theta) - \log p(\theta^*)$
			\State Sample $u \sim \text{U}([0, 1])$
			\If{$\log u \le \alpha(\theta, \theta^*)$}
			\State \Return $\theta^*$, $l^*$, $\alpha$
			\Else
			\State \Return $\theta$, $l$, $\alpha$
			\EndIf
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/stability.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/bifucdiagram.pdf}
		\end{minipage}
		\caption{\textbf{(left)}Bifurcation diagram of the Ricker map. \textbf{Dotted} lines correspond to unstable equilibria and \textbf{solid} lines to stable ones. \textbf{(right)} Bifurcation diagram of the Ricker Map.}
		\label{fig:stability}
	\end{figure}

	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/0value_ricker.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/stable_ricker.pdf}
		\end{minipage}
		\caption{Convergence towards the equilibria of the Ricker map}
		\label{fig:stab}
	\end{figure}

	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/oscill_ricker.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/4values_ricker.pdf}
		\end{minipage}
		\caption{\textbf{(left)} The population size oscillates before converging towards equilibrium. \textbf{(right)} The population sizes describes an orbit of length four.}
		\label{fig:oscill}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/rchange.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/initchange.pdf}
		\end{minipage}
		\caption{\textbf{(left)} Evolution of the population size when \textbf{(black)}$r=50$ and \textbf{(red)}$r=50.1$ with initial value $N_0=7$. \textbf{(right)} Evolution of the population size when \textbf{(black)}$N_0=7$ and \textbf{(red)}$N_0=7.1$ with $r=50$.}
		\label{fig:chaos}
	\end{figure}

	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/runningRicker.pdf}
		\end{minipage}
		\caption{Running time of Algorithm~\ref{pf}, averaged over 100 runs, for an increasing number of particles. In \textbf{red} is shown a linear intercept of the curve.}
		\label{fig:runningRicker}
	\end{figure}

	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/ESSRickerPrior.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/ESSRickerGamma.pdf}
		\end{minipage}
		\caption{\textbf{(left)} Evolution of the effective sample size using \textbf{(left)} the prior proposal, \textbf{(right)} our gamma approximation to the optimal proposal.}
		\label{fig:essRicker}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/mleRickerr.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/mleRickerphi.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/mleRickersigma.pdf}
		\end{minipage}
		\caption{Transect of the log-likelihood with respect to \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$.}
		\label{fig:transect}
	\end{figure}
	
	\begin{table}[htb]
		\centering
		\ra{1.3}
		\begin{tabular}{@{}ccccc@{}} \toprule
			Number of particles & Variance1 &  Variance2 & MSE1 & MSE2 \\ \midrule
			50 & 12.828163 &  9.675612 & 12.802 & 9.483\\ 
			100 & 10.44 & 7.99 & 10.34 & 8.19\\ 
			200 & 8.164184 & 5.214388 &  8.241 & 5.127\\ 
			500 & 4.349490 & 2.206633 & 4.825 & 2.185\\ 
			1000 & 3.490714 & 1.881224 & 3.661  & 2.228\\  \bottomrule
		\end{tabular}
		\caption{Variance and mean squared error of the maximum likelihood for r obtained using 50, 100, 200, 500, 1000 and 1500 particles}
		\label{table:mleR}
	\end{table}

	\begin{table}[htb]
		\centering
		\ra{1.3}
		\begin{tabular}{@{}ccccc@{}} \toprule
			Number of particles & Variance1 &  Variance2 & MSE1 & MSE2 \\ \midrule
			50 & 0.10 & 0.052 & 0.26 & 0.16\\
			100 & 0.054 & 0.041 & 0.18 & 0.17\\ 
			200 & 0.052 & 0.037 & 0.15 & 0.14\\ 
			500 & 0.041 & 0.025 & 0.17 & 0.14\\
			1000 & 0.021 & 0.018 & 0.17 & 0.14 \\ \bottomrule
		\end{tabular}
		\caption{Variance and mean squared error of the maximum likelihood for $\phi$ obtained using 50, 100, 200, 500, 1000 and 1500 particles}
		\label{table:mlePhi}
	\end{table}

	\begin{table}[htb]
		\centering
		\ra{1.3}
		\begin{tabular}{@{}ccccc@{}} \toprule
			Number of particles & Variance1 &  Variance2 & MSE1 ($10^{-3}$)& MSE2\\ \midrule
			50 & 2.61 & 2.32 & 2.58 & 2.51\\
			100 & 2.09 & 1.47 & 2.13 & 1.81\\
			200 & 2.04 & 1.41 &  2.3235  & 3.20\\ 
			500 & 1.32 & 1.00 & 1.73 & 1.78\\
			1000 & 0.83 & 0.74 & 1.33 & 1.71 \\  \bottomrule
		\end{tabular}
		\caption{Variance and mean squared error (MSE) of the maximum likelihood for $\sigma$ obtained using 50, 100, 200, 500 and 1000 particles. Values for variances and MSE are given in unit of $10^{-3}$}
		\label{table:mleSigma}
	\end{table}

	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/mleRicker_r_prior_500.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/mleRicker_r_gamma_500.pdf}
		\end{minipage}
		\caption{}
		\label{fig:comparisonR}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/traceRickerSameV1.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/traceRickerSameV2.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/traceRickerSameV3.pdf}
		\end{minipage}
		\caption{Traceplot of the samples from the posterior density of \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$.}
		\label{fig:traceplotDiag}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/acfRickerSameV1.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/acfRickerSameV2.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/acfRickerSameV3.pdf}
		\end{minipage}
		\caption{Autocorrelation function of the samples from the posterior density of \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$. The \textbf{(red)} is the theoretical asymptotic 95\% significance threshold.}
		\label{fig:acfDiag}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/runningRickerSame1.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/runningRickerSame2.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/runningRickerSame3.pdf}
		\end{minipage}
		\caption{Running means of the samples from the posterior density of \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$.}
		\label{fig:rmDiag}
	\end{figure}

\clearpage
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/gelmanRickerSame1.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/gelmanRickerSame2.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/gelmanRickerSame3.pdf}
		\end{minipage}
		\caption{Gelman-Rubin diagnostic for the samples from the posterior density of \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$.}
		\label{fig:gelmanDiag}
	\end{figure}
	
	\begin{table}[htb]
		\centering
		\ra{1.3}
		\begin{tabular}{@{}cccc@{}} \toprule
			Parameter & Fasiolo's MSE & LibBi MSE & our MSE\\ \midrule 
			$r$ & 0.0109 & 0.0080 &  0.0086 \\ 
			$\phi$ & $4 \ 10^{-4}$ & $1.0 \ 10^{-3}$ &  $7.3 \ 10^{-4}$ \\ 
			$\sigma$ & 0.0446 & 0.023 & 0.018  \\ \bottomrule
		\end{tabular}
		\caption{Median squared errors of the posterior means obtained by Fasiolo and in our study.}
		\label{table:mse}
	\end{table}

	\begin{table}[htb]
		\centering
		\ra{1.3}
		\begin{tabular}{@{}ccccc@{}} \toprule
			Method & Median &  Inter-quartile range & 1\textsuperscript{st} quartile & 3\textsuperscript{rd} quartile \\ \midrule 
			Fasiolo & 0.004 & 0.015 & 0.001 & 0.016\\ 
			LibBi & 0.0037 & 0.0051 &0.0018 & 0.0069 \\ 
			Our implementation & 0.0039 &  0.0063 & 0.0017 & 0.0080\\ \bottomrule
		\end{tabular}
		\caption{Values obtained for Fasiolo's custom error measure with the four methods.}
		\label{table:metric}
	\end{table}
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/acfLagopusV1.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/acfLagopusV2.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/acfLagopusV3.pdf}
		\end{minipage}
		\caption{Autocorrelation function of the samples from the posterior density of \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$. The \textbf{(red)} is the theoretical asymptotic 95\% significance threshold.}
		\label{fig:acfLagopus}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/runningLagopus1.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/runningLagopus2.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/runningLagopus3.pdf}
		\end{minipage}
		\caption{Running means of the samples from the posterior density of \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$.}
		\label{fig:rmLagopus}
	\end{figure}
	
	\clearpage
	\begin{figure}[htb]
		\centering
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/gelmanLagopus1.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/gelmanLagopus2.pdf}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
			\centering
			\includegraphics[width=0.97\linewidth]{/home/raphael/dissertation/figures/gelmanLagopus3.pdf}
		\end{minipage}
		\caption{Gelman-Rubin diagnostic for the samples from the posterior density of \textbf{(top left)} $r$, \textbf{(top right)} $\phi$ and \textbf{(bottom)} $\sigma$.}
		\label{fig:gelmanLagopus}
	\end{figure}
	
	\begin{algorithm}
		\caption{Random Weights Particle filter (RWPF)}\label{RWPF}
		\begin{algorithmic}[1]
			\Function{PARTICLE-FILTER}{N, M, T, $\theta$}
			\State $t \gets 1$
			\State $\hat{l}_0 \gets 0$
			\While{$t \le T$}
			\ForAll{$i \in \{1, \cdots, N\}$}
			\State Sample $s_t^{(i)} \sim q_s(s_t| n_{t-\tau} \theta)$
			\ForAll{$j \in \{1, \cdots, M\}$}
			\State Sample $r_t^{(j)} \sim q_r(r_t| n_{t-1} \theta)$
			\State $w_t^{(j)}(s_t^{(i)}) \gets \delta(n_t=r_t^{(j)}+s_t^{(i)})\frac{p(r_t^{(j)}| n_{t-1} \theta)}{q_r(r_t^{(j)}| n_{t-1} \theta)}$
			\EndFor
			\State $w_t^{(i)} \gets \frac{1}{M}\sum_{j=1}^{M}w_t^{(j)}(s_t^{(i)}) \frac{p(s_t^{(i)}| n_{t-\tau} \theta)}{q_s(s_t^{(i)}| n_{t-\tau} \theta)}$
			\EndFor
			\State $\hat{l}_t \gets \hat{l}_{t-1} - \log N + \log\sum_{i=1}^{N}w_t^{(i)}$
			\State Normalize $(w_t^{(0)}, \cdots, w_t^{(N)})$
			\State Set $\mathrm{ESS_t} \gets (\sum_{i=1}^{N}(\mathrm{W}_t^{(i)})^2)^{-1}$ // W denotes a normalized weight
			\EndWhile
			\item[]
			\Return $\hat{l}_T$,  $(\mathrm{ESS}_0, \cdots, \mathrm{ESS}_T)$
			\EndFunction
		\end{algorithmic}
	\end{algorithm}

\clearpage

	\bibliographystyle{plain}
	\bibliography{mybib}{}
	
\begin{appendices}
	\section{Derivations relative to the Noisily Observed Ricker Map} \label{KLRicker}
	In this section we present the minimization of the KL divergence between a Lognormal a Gamma distributions.
	We have
	\begin{equation*}
	D_{KL}(P||Q)(\alpha, \theta) = \int_{0}^{\infty}{p(z|\mu, \sigma^2)\log(\frac{p(z|\mu, \sigma^2)}{q(z|\alpha, \theta)})\mathrm{d}z}
	\end{equation*}
	where $p$ is the probability density function of a $\log\mathcal{N}(\mu, \sigma^2)$ and $q$ of a Gamma with shape $\alpha$ and scale $\theta$. \\
	Expanding we obtain:
	\begin{equation*}
	D_{KL}(P||Q)(\alpha, \theta) = C + \alpha\log\theta + \log\Gamma(\alpha) - (\alpha-1)\mathbb{E}_p[\log(Z)] + \frac{1}{\theta}\mathbb{E}_p[Z]
	\end{equation*}
	where $\mathbb{E}_p$ is the expectation with respect to the probability measure $p$.\\
	Therefore:
	\begin{equation*}
	\frac{\partial }{\partial \alpha}(D_{KL}(P||Q)) = \log(\theta) + \psi^{(0)}(\alpha)-\mathbb{E}_p[\log(Z)]
	\end{equation*}
	\begin{equation*}
	\frac{\partial }{\partial \theta}(D_{KL}(P||Q)) = \frac{\alpha}{\theta} - \frac{1}{\theta^2}\mathbb{E}_p[Z]
	\end{equation*}
	where $\psi^{(0)}$ is the digamma function.
	
	Since $\mathbb{E}_p[\log(Z)]=\mu$ and $\mathbb{E}_p[Z] = e^{\mu+\frac{\sigma^2}{2}}$, we finally have that, setting the partial derivatives to zero, the solutions satisfy:
	\[	\begin{cases}
	& \alpha=e^{\psi^{(0)}(\alpha)+\frac{\sigma^2}{2}} \\
	& \theta=\frac{1}{\alpha}e^{\mu+\frac{\sigma^2}{2}}
	\end{cases}\]
	
	If we use the first terms of the asymptotic expansion of the digamma function, i.e $\psi^{(0)}(\alpha) \approx \log(\alpha)-\frac{1}{2\alpha}$, we finally have $\alpha =\frac{1}{\sigma^2}$ and $\theta=\frac{1}{\alpha}e^{\mu+\frac{\sigma^2}{2}}$. \\
	
	In order to check if the solutions are indeed minima, we calculated the Hessian of $D_{KL}(P||Q)(\alpha, \theta)$ i.e
	\begin{equation*}
	\text{H} = \begin{pmatrix}
	\frac{\strut \partial^2 D_{KL}(P||Q)}{\strut \partial \alpha^2} & \frac{\strut \partial^2 D_{KL}(P||Q) }{\strut \partial \theta \partial \alpha} \\
	\frac{\strut \partial^2 D_{KL}(P||Q)}{\strut \partial \theta \partial \alpha} & \frac{\strut \partial^2 D_{KL}(P||Q)}{\strut \partial \theta^2} 
	\end{pmatrix} =
	\begin{pmatrix}
	\psi^{(1)}(\alpha) & \frac{1}{\theta} \\
	\frac{1}{\theta} & -\frac{\alpha}{\theta^2}+\frac{2e^{\mu +\frac{\sigma^2}{2}}}{\theta^3}
	\end{pmatrix}
	\end{equation*}
	Straightforward calculations yield that the sign of the determinant of the above Hessian is such that $\text{sign}(\det{H}) = \psi^{(1)}(\alpha)(\frac{2}{\theta^3}e^{\mu +\frac{\sigma^2}{2}} - \alpha) - 1$.
	
	\section{Derivations relative to Nicholson's Blowfly Experiments} \label{AppendBlowfly}
	\subsection{Derivation of $p(r_t|n_{t-\tau}, \theta)$} \label{rDensity}
	In this section we derive the density $p(r_t|n_{t-\tau}, \theta)$ which is required to calculate the importance weights in Algorithm~\ref{RWPF}.
	We have:
	\begin{align*}
	p(r_t | n_{t-\tau}, \theta) & = \int_{0}^{\infty}p_r(r_t, u_t | n_{t-\tau}, \theta)\ \mathrm{d}u_t \\
	& = \int_{0}^{\infty}p_r(r_t | e_t, n_{t-\tau}, \theta)p(u_t | n_{t-\tau}, \theta)\ \mathrm{d}e_t \\
	& = \int_{0}^{\infty}p_r(r_t | u_t, n_{t-\tau}, \theta)p(u_t)\ \mathrm{d}u_t
	\end{align*}
	Since $u_t$ is independent of $n_{1:t}$
	Replacing $p(r_t | u_t, n_{t-\tau}, \theta)$ and $p(u_t)$ by their analytical expression:
	\begin{align*}
	p(r_t | n_{t-\tau}, \theta) & = \int_{0}^{\infty}e^{-\beta(n_{t-\tau})e_t}\frac{(\beta(n_{t-\tau})u_t)^{r_t}}{\Gamma(r_t+1)}\frac{\alpha^\alpha}{\Gamma(\alpha)}u_t^{\alpha-1}e^{-\alpha u_t}\ \mathrm{d}u_t \\
	& = \frac{\beta(n_{t-\tau})^{r_t}}{\Gamma(r_t+1)}\frac{\alpha^\alpha}{\Gamma(\alpha)}\int_{0}^{\infty}e^{-(\beta(n_{t-\tau})+\alpha)u_t}u_t^{r_t+\alpha-1}\ \mathrm{d}u_t \\
	& = \frac{\beta(n_{t-\tau})^r_t}{\Gamma(r_t+1)}\frac{\alpha^\alpha}{\Gamma(\alpha)}\frac{\Gamma(r_t+\alpha)}{(\beta(n_{t-\tau})+\alpha)^{r_t+\alpha}} \\
	& = \binom{r_t + \alpha - 1}{r_t}(\frac{\beta(n_{t-\tau})}{\beta(n_{t-\tau}) + \alpha})^{r_t}(\frac{\alpha}{\beta(n_{t-\tau}) + \alpha})^\alpha
	\end{align*}
	where $\beta(n_{t-\tau}) = Pn_{t-\tau}e^{-\frac{n_{t-\tau}}{N_0}}$ and $\alpha = \sigma_p^{-2}$. \\
	The last equation is the density of a Negative-Binomial distribution with parameter $\frac{\alpha}{\beta(n_{t-\tau}) + \alpha}$ and $\alpha$.
	
	\subsection{Derivation of $p(s_t|n_{t-1}, \theta)$} \label{sDensity}
	In this section we derive the density $p(s_t|n_{t-1}, \theta)$ which is required to calculate the importance weights in Algorithm~\ref{RWPF}.
	We have:
	\begin{align*}
	p(s_t | n_{t-1}) & = \int_{0}^{\infty}p(s_t, v_t | n_{t-1})\ \mathrm{d}v_t \\
	& = \int_{0}^{\infty}p(s_t | v_t, n_{t-1})p(v_t)\ \mathrm{d}v_t \\
	\end{align*}
	Since $v_t$ is independent of $n_{1:t}$
	Replacing $p(s_t | v_t, n_{t-1}, \theta)$ and $p(v_t)$ by their analytical expression:
	\begin{align*}
	p(s_t | n_{t-1}) & = \binom{n_{t-1}}{s_t}\frac{\alpha^\alpha}{\Gamma(\alpha)} \int_{0}^{\infty}e^{-\delta\epsilon_t s_t}(1-e^{-\delta\epsilon_t})^{n_{t-1}-s_t}\epsilon_t^{\alpha-1}e^{-\alpha\epsilon_t}\ \mathrm{d}\epsilon_t \\
	& = \binom{n_{t-1}}{s_t}\frac{\alpha^\alpha}{\Gamma(\alpha)}\int_{0}^{\infty}e^{-(\delta s_t+\alpha)v_t }(1-e^{-\delta v_t})^{n_{t-1}-s_t}v_t^{\alpha-1}\ \mathrm{d}v_t
	\end{align*}
	Using the fact that $(1-e^{-\delta v_t})^{n_{t-1}-s_t} = \sum_{k=0}^{n_{t-1}-s_t}\binom{n_{t-1}-s_t}{k}(-1)^ke^{-\delta k v_t}$ and swapping the integral and the sum (since each $\int_{0}^{\infty}e^{-(\delta s_t+\alpha + k\delta)v_t }v_t^{\alpha-1}\ \mathrm{d}v_t < \infty$ because $\delta,  s_t, \alpha > 0$ and $\alpha > 1$) we have:
	\begin{align*}
	p(s_t | n_{t-1}) & = \binom{n_{t-1}}{s_t}\frac{\alpha^\alpha}{\Gamma(\alpha)}\sum_{k=0}^{n_{t-1}-s_t}\binom{n_{t-1}-s_t}{k}(-1)^k \ \int_{0}^{\infty}e^{-(\delta s_t+\alpha + k\delta)v_t }v_t^{\alpha-1}\ \mathrm{d}v_t \\
	& = \binom{n_{t-1}}{s_t}\alpha^\alpha\sum_{k=0}^{n_{t-1}-s_t}\binom{n_{t-1}-s_t}{k}\frac{(-1)^k}{(\delta(s_t+k)+\alpha)^\alpha}
	\end{align*}
\end{appendices}
	
\end{document}